{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:43:43.534515Z",
     "iopub.status.busy": "2021-11-24T00:43:43.533918Z",
     "iopub.status.idle": "2021-11-24T00:43:43.633639Z",
     "shell.execute_reply": "2021-11-24T00:43:43.632895Z",
     "shell.execute_reply.started": "2021-11-24T00:43:43.534421Z"
    },
    "id": "ZCUQzWYvy9cL",
    "outputId": "8190947e-aa4b-4b61-f51b-37b6d8f7fd6e"
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3FoTyRa9pLu"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:43:43.635665Z",
     "iopub.status.busy": "2021-11-24T00:43:43.635390Z",
     "iopub.status.idle": "2021-11-24T00:43:48.586003Z",
     "shell.execute_reply": "2021-11-24T00:43:48.584573Z",
     "shell.execute_reply.started": "2021-11-24T00:43:43.635629Z"
    },
    "id": "f_sOaV1Y8NsL",
    "outputId": "6ceb6bc5-79b4-491e-a404-e56f48a517ee"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLb-N5JzUUQS"
   },
   "source": [
    "### Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:43:48.587536Z",
     "iopub.status.busy": "2021-11-24T00:43:48.587285Z",
     "iopub.status.idle": "2021-11-24T00:43:48.592655Z",
     "shell.execute_reply": "2021-11-24T00:43:48.591994Z",
     "shell.execute_reply.started": "2021-11-24T00:43:48.587501Z"
    },
    "id": "C7HYua8HUHIj"
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dataset folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:43:48.595213Z",
     "iopub.status.busy": "2021-11-24T00:43:48.594724Z",
     "iopub.status.idle": "2021-11-24T00:43:48.602707Z",
     "shell.execute_reply": "2021-11-24T00:43:48.601974Z",
     "shell.execute_reply.started": "2021-11-24T00:43:48.595177Z"
    },
    "id": "jSmn5M8_PyJ1"
   },
   "outputs": [],
   "source": [
    "# Dataset folders \n",
    "dataset_dir = '../input/anndl-dataset'\n",
    "training_dir = os.path.join(dataset_dir, 'training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvjjNtBV_jBQ"
   },
   "source": [
    "### Class labels: \n",
    "\n",
    "1:Apple \n",
    "\n",
    "2:Blueberry \n",
    "\n",
    "3:Cherry \n",
    "\n",
    "4:Corn \n",
    "\n",
    "5:Grape \n",
    "\n",
    "6:Orange \n",
    "\n",
    "7:Peach \n",
    "\n",
    "8:Pepper \n",
    "\n",
    "9:Potato \n",
    "\n",
    "10:Raspberry \n",
    "\n",
    "11:Soybean \n",
    "\n",
    "12:Squash \n",
    "\n",
    "13:Strawberry \n",
    "\n",
    "14:Tomato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot example images from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:43:48.605889Z",
     "iopub.status.busy": "2021-11-24T00:43:48.605238Z",
     "iopub.status.idle": "2021-11-24T00:43:56.846590Z",
     "shell.execute_reply": "2021-11-24T00:43:56.845857Z",
     "shell.execute_reply.started": "2021-11-24T00:43:48.605854Z"
    },
    "id": "-UrdOzQ2-Jqe",
    "outputId": "7d552cc1-e475-47e1-dfae-9193090604ea"
   },
   "outputs": [],
   "source": [
    "# Plot example images from dataset\n",
    "labels = ['Apple',              # 0\n",
    "          'Blueberry',          # 1\n",
    "          'Cherry',             # 2\n",
    "          'Corn',               # 3\n",
    "          'Grape',              # 4\n",
    "          'Orange',             # 5\n",
    "          'Peach',              # 6\n",
    "          'Pepper',             # 7\n",
    "          'Potato',             # 8\n",
    "          'Raspberry',          # 9\n",
    "          'Soybean',            # 10\n",
    "          'Squash',             # 11\n",
    "          'Strawberry',         # 12\n",
    "          'Tomato']             # 13\n",
    "\n",
    "num_row = len(labels)//3\n",
    "num_col = len(labels)//num_row\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(3*num_row,5*num_col))\n",
    "for i in range(num_row*num_col):\n",
    "    if i < len(labels):\n",
    "        class_imgs = next(os.walk('{}/training/{}/'.format(dataset_dir, labels[i])))[2]\n",
    "        class_img = class_imgs[0]\n",
    "        img = Image.open('{}/training/{}/{}'.format(dataset_dir, labels[i], class_img))\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.set_title('{}'.format(labels[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader + Data Augmentation\n",
    "\n",
    "##### ImageDataGenerator allows to perform data augmentation\n",
    "\n",
    "```\n",
    "tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=False, samplewise_center=False,\n",
    "    featurewise_std_normalization=False, samplewise_std_normalization=False,\n",
    "    zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0,\n",
    "    height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n",
    "    channel_shift_range=0.0, fill_mode='nearest', cval=0.0,\n",
    "    horizontal_flip=False, vertical_flip=False, rescale=None,\n",
    "    preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:43:56.847891Z",
     "iopub.status.busy": "2021-11-24T00:43:56.847648Z",
     "iopub.status.idle": "2021-11-24T00:43:56.851943Z",
     "shell.execute_reply": "2021-11-24T00:43:56.851289Z",
     "shell.execute_reply.started": "2021-11-24T00:43:56.847861Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-centering and std-normalization of the training set is not performed becouse of the pre-processing on vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:43:56.854030Z",
     "iopub.status.busy": "2021-11-24T00:43:56.853537Z",
     "iopub.status.idle": "2021-11-24T00:44:04.766880Z",
     "shell.execute_reply": "2021-11-24T00:44:04.766154Z",
     "shell.execute_reply.started": "2021-11-24T00:43:56.853994Z"
    },
    "id": "DcWi9Iat96uk",
    "outputId": "d846d3d1-ff89-4f08-d8ab-88c87039d901"
   },
   "outputs": [],
   "source": [
    "# Images are divided into folders, one for each class. \n",
    "# If the images are organized in such a way, we can exploit the \n",
    "# ImageDataGenerator to read them from disk.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of ImageDataGenerator with Data Augmentation\n",
    "# no rescale value <- vgg16 automatic rescale the input in the first layer\n",
    "training_data_gen = ImageDataGenerator(#featurewise_center=True, \n",
    "                                    #featurewise_std_normalization=True, \n",
    "                                    rotation_range=30, \n",
    "                                    height_shift_range=50, \n",
    "                                    width_shift_range=50, \n",
    "                                    zoom_range=0.3, \n",
    "                                    horizontal_flip=True, \n",
    "                                    vertical_flip=True, \n",
    "                                    fill_mode='reflect', \n",
    "                                    preprocessing_function=preprocess_input, \n",
    "                                    validation_split=0.15) \n",
    "\n",
    "# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n",
    "training_set_gen = training_data_gen.flow_from_directory(directory=training_dir, \n",
    "                                                   target_size=(256,256), \n",
    "                                                   color_mode='rgb', \n",
    "                                                   classes=None, # can be set to labels \n",
    "                                                   class_mode='categorical', \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, \n",
    "                                                   subset='training', \n",
    "                                                   seed=seed)\n",
    "\n",
    "validation_set_gen = training_data_gen.flow_from_directory(directory=training_dir, \n",
    "                                                   target_size=(256,256), \n",
    "                                                   color_mode='rgb', \n",
    "                                                   classes=None, # can be set to labels \n",
    "                                                   class_mode='categorical', \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=False, \n",
    "                                                   subset='validation',\n",
    "                                                   seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigned label and target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:04.768603Z",
     "iopub.status.busy": "2021-11-24T00:44:04.768204Z",
     "iopub.status.idle": "2021-11-24T00:44:04.776963Z",
     "shell.execute_reply": "2021-11-24T00:44:04.776056Z",
     "shell.execute_reply.started": "2021-11-24T00:44:04.768569Z"
    },
    "id": "wothhodRGqqo",
    "outputId": "a7ca8fb0-ebf6-4c5f-8e5a-68fa080a184a"
   },
   "outputs": [],
   "source": [
    "print(\"Assigned labels\")\n",
    "print(training_set_gen.class_indices)\n",
    "print()\n",
    "print(\"Target classes\")\n",
    "print(training_set_gen.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sample from dataset and show info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:04.779203Z",
     "iopub.status.busy": "2021-11-24T00:44:04.778608Z",
     "iopub.status.idle": "2021-11-24T00:44:04.787381Z",
     "shell.execute_reply": "2021-11-24T00:44:04.786437Z",
     "shell.execute_reply.started": "2021-11-24T00:44:04.779164Z"
    },
    "id": "5D5ln0cHVL2b"
   },
   "outputs": [],
   "source": [
    "def get_next_batch(generator):\n",
    "    batch = next(generator)\n",
    "\n",
    "    image = batch[0]\n",
    "    target = batch[1]\n",
    "\n",
    "    print(\"(Input) image shape:\", image.shape)\n",
    "    print(\"Target shape:\",target.shape)\n",
    "\n",
    "    # Visualize only the first sample\n",
    "    image = image[0]\n",
    "    target = target[0]\n",
    "    target_idx = np.argmax(target)\n",
    "    print()\n",
    "    print(\"Categorical label:\", target)\n",
    "    print(\"Label:\", target_idx)\n",
    "    print(\"Class name:\", labels[target_idx])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(np.uint8(image))\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:04.791701Z",
     "iopub.status.busy": "2021-11-24T00:44:04.791440Z",
     "iopub.status.idle": "2021-11-24T00:44:06.137122Z",
     "shell.execute_reply": "2021-11-24T00:44:06.136437Z",
     "shell.execute_reply.started": "2021-11-24T00:44:04.791676Z"
    },
    "id": "WJ6q1cJLalh7",
    "outputId": "c15818bb-47e7-41e0-870d-9ba4d0087d0e"
   },
   "outputs": [],
   "source": [
    "# Get a sample from dataset and show info\n",
    "_ = get_next_batch(training_set_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr9CX7CYdBg_"
   },
   "source": [
    "### Models metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:06.138955Z",
     "iopub.status.busy": "2021-11-24T00:44:06.138320Z",
     "iopub.status.idle": "2021-11-24T00:44:06.143365Z",
     "shell.execute_reply": "2021-11-24T00:44:06.142709Z",
     "shell.execute_reply.started": "2021-11-24T00:44:06.138918Z"
    },
    "id": "7YcxBMJhl4EM"
   },
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:06.145294Z",
     "iopub.status.busy": "2021-11-24T00:44:06.144562Z",
     "iopub.status.idle": "2021-11-24T00:44:09.643889Z",
     "shell.execute_reply": "2021-11-24T00:44:09.643076Z",
     "shell.execute_reply.started": "2021-11-24T00:44:06.145257Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download and plot the VGG16 model\n",
    "supernet = tfk.applications.VGG16(\n",
    "    include_top=False, # with false we remove the classifier from vgg, taking only the features extraction part\n",
    "    weights=\"imagenet\", # means that we want the weights of the network trained on the imagenet\n",
    "    input_shape=input_shape # is the shape of the input with witch we will train the supernet\n",
    ")\n",
    "supernet.summary()\n",
    "tfk.utils.plot_model(supernet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supernetwork as feature extractor (1° phase, all the layers frozen)\n",
    "#### (in the following 2° phase: Fine Tuning)\n",
    "##### In this first phase we use the supernet as feature extractor, freezing all the imported layer. In the following and last phase we perform Fine Tuning, setting to learnable the last layer of the imported CNN, and keep freezeng the first N layer [N must be decided])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:09.646252Z",
     "iopub.status.busy": "2021-11-24T00:44:09.645619Z",
     "iopub.status.idle": "2021-11-24T00:44:09.757689Z",
     "shell.execute_reply": "2021-11-24T00:44:09.757003Z",
     "shell.execute_reply.started": "2021-11-24T00:44:09.646210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the supernet as feature extractor\n",
    "supernet.trainable = False\n",
    "\n",
    "# Input layer\n",
    "inputs = tfk.Input(shape=input_shape)\n",
    "\n",
    "# Supernet layer\n",
    "x = supernet(inputs)\n",
    "\n",
    "# MaxPooling layer\n",
    "x = tfkl.MaxPooling2D(name='MaxPooling2D')(x)\n",
    "\n",
    "# Flattening layer\n",
    "x = tfkl.Flatten(name='Flatten')(x)\n",
    "\n",
    "# 1° hidden layer\n",
    "x = tfkl.Dense(256, \n",
    "               activation='relu', \n",
    "               kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n",
    "x = tfkl.Dropout(0.3, seed=seed)(x)\n",
    "\n",
    "# 2° hidden layer\n",
    "x = tfkl.Dense(256, \n",
    "               activation='relu', \n",
    "               kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n",
    "x = tfkl.Dropout(0.3, seed=seed)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = tfkl.Dense(len(labels), \n",
    "                     activation='softmax', \n",
    "                     kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n",
    "\n",
    "\n",
    "# Connect input and output through the Model class\n",
    "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model, adam learning rate: 0.001 (in the second phase will be shrinked)\n",
    "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(0.001), metrics='accuracy')\n",
    "tl_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function to create callbacks (for checkpoint and earlystopping) and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:09.759154Z",
     "iopub.status.busy": "2021-11-24T00:44:09.758893Z",
     "iopub.status.idle": "2021-11-24T00:44:09.772316Z",
     "shell.execute_reply": "2021-11-24T00:44:09.771653Z",
     "shell.execute_reply.started": "2021-11-24T00:44:09.759109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to create folders and callbacks for training\n",
    "from datetime import datetime\n",
    "\n",
    "def create_folders_and_callbacks(model_name):\n",
    "\n",
    "    exps_dir = os.path.join('transfer_learning_experiments')\n",
    "    if not os.path.exists(exps_dir):\n",
    "        os.makedirs(exps_dir)\n",
    "\n",
    "    now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "    exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    # Model last checkpoint\n",
    "    # ----------------\n",
    "    ckpt_dir = os.path.join(exp_dir, 'lastckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'lastcp.ckpt'), \n",
    "                                                     save_weights_only=False, # True to save only weights\n",
    "                                                     save_best_only=False) # True to save only the best epoch \n",
    "    callbacks.append(ckpt_callback)\n",
    "\n",
    "    # Model best checkpoint\n",
    "    # ----------------\n",
    "    ckpt_dir = os.path.join(exp_dir, 'bestckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'bestcp.ckpt'), \n",
    "                                                     save_weights_only=False, # True to save only weights\n",
    "                                                     save_best_only=True) # True to save only the best epoch \n",
    "    callbacks.append(ckpt_callback)\n",
    "\n",
    "    # Visualize Learning on Tensorboard\n",
    "    # ---------------------------------\n",
    "    tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "    if not os.path.exists(tb_dir):\n",
    "        os.makedirs(tb_dir)\n",
    "\n",
    "    # By default shows losses and metrics for both training and validation\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                               profile_batch=0,\n",
    "                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n",
    "    callbacks.append(tb_callback)\n",
    "\n",
    "    # Early Stopping\n",
    "    # --------------\n",
    "    es_callback = tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=25, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:09.775506Z",
     "iopub.status.busy": "2021-11-24T00:44:09.774853Z",
     "iopub.status.idle": "2021-11-24T00:44:09.784398Z",
     "shell.execute_reply": "2021-11-24T00:44:09.783582Z",
     "shell.execute_reply.started": "2021-11-24T00:44:09.775469Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set class weight\n",
    "\n",
    "class_weights = [1.2817,2.7115,2.172,1.05,0.8685,0.7244,1.2961,1.6553,1.7686,4.7965,0.7836,2.2061,1.8816,0.2224]\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-24T00:44:09.786100Z",
     "iopub.status.busy": "2021-11-24T00:44:09.785717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create folders and callbacks\n",
    "tl_callbacks = create_folders_and_callbacks(model_name='TransferLearning_first_step')\n",
    "\n",
    "# Train the model\n",
    "tl_history = tl_model.fit(x = training_set_gen, \n",
    "                          class_weight = class_weights, \n",
    "                          batch_size = batch_size, \n",
    "                          epochs = epochs, \n",
    "                          validation_data = validation_set_gen, \n",
    "                          callbacks = tl_callbacks \n",
    "                         ).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(standard_history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(standard_history['val_loss'], label='Standard', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')\n",
    "plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(standard_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(standard_history['val_accuracy'], label='Standard', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')\n",
    "plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on the validaion set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model = tfk.models.load_model('TransferLearningModel')\n",
    "# Predict the test set with the CNN\n",
    "predictions = tl_model.predict(validation_set_gen)\n",
    "predictions.shape\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(validation_set_gen, axis=-1), np.argmax(predictions, axis=-1))\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
    "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm.T, xticklabels=list(labels.values()), yticklabels=list(labels.values()))\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "tl_model.save('TransferLearningModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning (2° phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "ft_model = tfk.models.load_model('TransferLearningModel')\n",
    "ft_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we keep frozen only the first N layer of the supernetwork, and set to trainable the other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze first N layers, e.g., until 14th\n",
    "for i, layer in enumerate(ft_model.get_layer('vgg16').layers[:14]):\n",
    "    layer.trainable=False\n",
    "for i, layer in enumerate(ft_model.get_layer('vgg16').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "ft_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now re-compile the model shrinking the learning rate, to not \"destroy\" supernetwork weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-4), metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders and callbacks\n",
    "ft_callbacks = create_folders_and_callbacks(model_name='Fine_Tuning_2_step')\n",
    "\n",
    "# Fine-tune the model\n",
    "ft_history = ft_model.fit(x = training_set_gen, \n",
    "                          class_weight = class_weights, \n",
    "                          batch_size = batch_size, \n",
    "                          epochs = ephocs, \n",
    "                          validation_data = validation_set_gen, \n",
    "                          callbacks = ft_callbacks \n",
    "                         ).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(standard_history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(standard_history['val_loss'], label='Standard', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(tl_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')\n",
    "plt.plot(tl_history['val_loss'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
    "plt.plot(ft_history['loss'], alpha=.3, color='#2ABC3D', linestyle='--')\n",
    "plt.plot(ft_history['val_loss'], label='Fine Tuning', alpha=.8, color='#2ABC3D')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(standard_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(standard_history['val_accuracy'], label='Standard', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')\n",
    "plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
    "plt.plot(ft_history['accuracy'], alpha=.3, color='#2ABC3D', linestyle='--')\n",
    "plt.plot(ft_history['val_accuracy'], label='Fine Tuning', alpha=.8, color='#2ABC3D')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "predictions = ft_model.predict(validation_set_gen)\n",
    "predictions.shape\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(validation_set_gen, axis=-1), np.argmax(predictions, axis=-1))\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
    "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm.T, xticklabels=list(labels.values()), yticklabels=list(labels.values()))\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save('FineTuningModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
